name: "[Update] News Database"

# Run cron job every day at midnight (UTC)
on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch: 

jobs:
  update-news-database:
    runs-on: ubuntu-latest
    steps:
      # Step 1: Check out the code from the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Installing dependencies
      - name: Installing dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y mysql-server
          sudo apt-get install jq
          sudo service mysql start
          
      # Step 3: 
      - name: Set up Docker Compose
        run: |
          docker compose version

      # Step 4:
      - name: Start MySQL Service
        env:
            MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD }}
            MYSQL_DATABASE: ${{ secrets.MYSQL_DATABASE }}
        run: |
          docker compose up -d
          docker compose ps

      # Step 5:
      - name: Wait for MySQL to be ready
        run: |
          until docker exec mysql-db mysql -u root -p${{ secrets.MYSQL_ROOT_PASSWORD }} -e 'SHOW DATABASES;' > /dev/null 2>&1; do
            echo "Waiting for MySQL to be ready..."
            docker logs mysql-db
            sleep 5
          done
          echo "MySQL is ready!"

      # Step 6:
      - name: Initialize DB
        env:
            MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD }}
            MYSQL_DATABASE: ${{ secrets.MYSQL_DATABASE }}
        run: |
          bash ./db/initializeDatabase.sh

      # Step 7:
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      # Step 8:
      - name: Installing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 9:
      - name: Scrape News and dump json in output folder under ./output/news-<stock>-data.json
        env:
            NEWS_API: ${{ secrets.NEWS_API }}
            NEWS_DATA: ${{ secrets.NEWS_DATA }}
        run: |
          python3 scrapers/newsScraper.py

      # Step 10:
      - name: Run adding to DB file
        env:
            MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD }}
            MYSQL_DATABASE: ${{ secrets.MYSQL_DATABASE }}
        run: |
          bash db/newsUpdateDatabase.sh

      # Step 11:
      - name: Dump News DB data in ./db/news/news-<stock>-data.sql
        env:
          MYSQL_ROOT_PASSWORD: ${{ secrets.MYSQL_ROOT_PASSWORD }}
          MYSQL_DATABASE: ${{ secrets.MYSQL_DATABASE }}
        run: |
          bash db/newsDumpData.sh
      
      # Step 12:
      - name: Tear down Docker Compose
        run: |
          docker compose down 

      # Step 13:
      - name: Remove JSON output file
        run: |
          rm output/*.json
      
      # Step 14:
      - name: Configure Git
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
      
      # Step 15:
      - name: Commit changes
        run: |
          git add .
          git commit -m "Updated News Database dump" || echo "No changes to commit"
      
      # Step 16:
      - name: Push changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git push origin HEAD:main
